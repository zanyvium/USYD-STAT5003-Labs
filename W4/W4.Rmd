---
title: "W4 STAT5003 RMD"
author: "Victor Z. Nygaard, vnyg7406"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{verbatim}
   - \usepackage[language]{babel}
   - \usepackage[encoding]{inputenc}
   - \usepackage{hyperref}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \usepackage{amssymb}
   - \usepackage{mathtools}
   - \usepackage{nicefrac}
   - \usepackage{fullpage}
   - \usepackage{stmaryrd}
   - \usepackage{aligned-overset}
   - \usepackage{pdfpages}
date: "Last compiled on `r format(Sys.time(), '%d. %B, %Y')`"
# Hvordan får  vi den til at printe February i stedet for februar??????
output: html_document
---

------------------------------------------------------------------------

```{=html}
<!-- RMD tips:
1. CTRL+SHIFT+C RMD-comments-out the selected lines, with in a standard HTML comment-out format. 

2. CTRL+ALT+I inserts a new r codechunck

3. Pressing CTRL+SHIFT+ENTER when over a code chunck gives you a preview of the results of the chunck

4. CTRL+SHIFT+K gives you a preview of the entire resulting HTML file.

5. Note the different results of '#HS 1' and 
'# HS 1' (without the '') in the output

6. It is possible to compile regular R-scripts into Rmd files - this is done by pressing CTRL+SHIFT+K while attending any R-script. <<<- Though this apparently doesn't work for HS Problems.R for some reason!?!?!? ->>>

7. Selective use of the echo=c(...) option within code chuncks allows assignment of a variable, to show the assignment in the knitted document, and showing the value of the assignment seamlessly as well - see HS2.3

8. It is possible to include results of R-analysis such as summary statistics in LaTeX-equations in RMD, see HS2.3

9. Adding fig.align="center" to a code chunk centers any figures generated by the chunck.

9.1 note that properties of codechunks seem casesensitive; fig.align="center" centers a figure, but fig.align="Center" (with capital C) doesn't

10. A new subtitle needs a blank line before itself: 
'works:

#### HS 7
'

'doesn't:
blablabla
#### HS 7
'

'doesn't either:
<!-- blablabla ->
#### HS 7
'
11. Pressing F7 when marking, or hovering over a word will spellcheck the word

12. CTRL + - (minus) zooms out, CTRL + + (plus) zooms in

13. CTRL + D Deletes the current line, or current selection of lines

14. THE FOLLOWING SOURCE EDITOR FOLDING METHODS:
14.1 Collapse current fold: ALT + L
14.1.1: Expand current fold: SHIFT + ALT + L
14.2 Collapse "all" subfolds: ALT + O <- !?!! Note that this leaves a small letter 'o' in the text !!?!
14.2.1 Expand "all" subfolds: SHIFT + ALT + O
14.3 Collapse all other folds: ALT + 0 (zero)

15. SHIFT + ALT + J allows you to jump to specific parts of the document

16. Writing a new line with '...' will cause all previous output to be hidden in the knittet document

17. Writing (q<-5) around R code, will both assign and print the code upon assignment 

18. Note that 'attach' only has the scope of the current R-chunck.

19. One way to get pdf printout is to compile a html-printout, and then, in-browser, 'print' the HTML page as a pdf.

20. CTRL + SHIFT + M gives the pipe operator.

21. Pressing CTRL + F3 searches on the selected word.

22. CTRL

23. ALT + SHIFT + DOWN copies a line to below.
-->
```
<!-- ---?--- How do I create a closeable Rmd section, such that I do not have to scroll through the LaTeX commands each time? - !!! Can be done with '-----' through this also creates a line in the knittet document. -->

<!-- How do I publish and share the HTML as a viewable (and linkable) website - this can be done through github? -->

<!-- How can I share R markdown files such that multiple people can edit them at the same time? -->

<!-- Do we need parindent controls as in LaTeX? -->

<!-- Use of the cache function to reduce recompile times -->

<!-- How to close current subsection with a keyboard shortcut? How to close subsubsections,...? - !!!See RMD tip 14!!! -->

<!-- Chunk naming? -->

<!-- How to define variables such that they have scope within their own ## segment? -->

<!-- How do I delete all non-needed variables for each new section in R??? -->

<!-- LaTeX commands -->

\newcommand{\C}{\mathbb{C}}

<!--- Komplekse tal --->

\newcommand{\R}{\mathbb{R}}

<!--- Reelle tal--->

\newcommand{\Q}{\mathbb{Q}}

<!---Rationelle tal--->

\newcommand{\Z}{\mathbb{Z}}

<!---Hele tal--->

\newcommand{\N}{\mathbb{N}}

<!---Naturlige tal--->

\newcommand{\E}{\mathbb{E}}

<!---mean--->

\newcommand{\F}{\mathbb{F}}

<!---Baggrundsrum sigma-alg--->

\newcommand{\B}{\mathbb{B}}

<!---Borel sigma--->

\newcommand{\K}{\mathbb{K}}

<!---Generel field--->

\newcommand{\RB}{\overline{\R}}

<!---Udvidede reelle tal--->

\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\BR}{\mathcal{B}\left(\R\right)}

<!---Borel på Reelle tal -->

\newcommand{\BRB}{\mathcal{B}\left(\RB\right))}

<!---Borel på udvidede reelle tal -->


\newcommand{\mf}[1]{\mathfrak{#1}} 
\newcommand{\mcG}[2]{\mathcal{#1}^1(#2)} 
\newcommand{\mcGG}[4]{\mathcal{#1}_{#3}^{#2}(#4)}
\newcommand{\GMR}{\left(X,\ms{A},\mu\right)}

<!---Generelt målrum -->

\newcommand{\PBS}{\lrp{\Omega,\F, P}}

<!--- Probability background space -->

\newcommand{\RMR}{\left(\R,\BR, \lambda\right)}

<!---Reelt målrum, m. Borel, og lebesgue mål. -->

<!---L_p spaces on [0,1] with m -->


\newcommand{\Lp}[1]{L_{#1}\lrp{\lrs{0,1},m}} 
\newcommand{\mclxy}{\mc{L}\lrp{X,Y}}
<!---Bounded linear functionals from X to Y -->

\newcommand{\mckxy}{\mc{K}\lrp{X,Y}}

<!---Compact Bounded linear functionals from X to Y -->

\newcommand{\mssr}{\ms{S}(\R)}

<!---The Schwartz space on $\R$ -->

<!---Arrows -->

\newcommand{\ra}{\rightarrow}

<!---Konvergens pil højre -->

\newcommand{\nra}{\nrightarrow}

<!---ikke Konvergens pil højre -->

\newcommand{\la}{\leftarrow}

<!---Konv pil venstre -->

\newcommand{\nla}{\nleftarrow}

<!---ikke Konvergens pil venstre -->

\newcommand{\lra}{\leftrightarrow}

<!---højre venstre pil -->

\newcommand{\nlra}{\nleftrightarrow}

<!---ikke højre venstre pil -->

\newcommand{\hra}{\hookrightarrow}

<!---Injektiv  pil højre -->

\newcommand{\Ra}{\Rightarrow}

<!---Implikations pil højre -->

\newcommand{\Lra}{\Leftrightarrow}

<!---Bi-implikations pil -->

\newcommand{\Uda}{\Updownarrow}

<!---Bi-implikations pil (op og ned) -->

\newcommand{\Da}{\Downarrow}

<!---implikations pil (ned) -->

\newcommand{\rhpu}{\rightharpoonup}

<!---Weak convergence in Hilbert spaces -->

<!-- LHS & RHS calculations -->

\newcommand{\swel}{\overset{\swarrow}{=}}

<!---Continue calculation on left hand side with equality -->

\newcommand{\sweq}{\overset{\swarrow}{\equiv}}

<!---Continue calculation on left hand side with equivalence -->

\newcommand{\seel}{\overset{\searrow}{=}}

<!---Continue calculation on right hand side with equality -->

\newcommand{\seeq}{\overset{\searrow}{\equiv}}

<!---Continue calculation on right hand side with equivalence -->

\newcommand{\inse}{\overset{\cdot}{=}}

<!--- Insert values in calculation -->

\newcommand{\eqd}{\overset{d.}{=}}

\newcommand{\PMX}{\mc{P}\left(X\right)}

<!---Potensmængde af X -->

\newcommand{\comp}{\mathsf{c}}

<!---Set compliment -->

\newcommand{\sm}{\setminus}

<!---mængdedifferens -->

<!--- Parenteser --->

\newcommand{\lrp}[1]{\mathopen{}\left({#1}\right)\mathclose{}}

<!-- \left("STUFF"\right) -->

\newcommand{\lrc}[1]{\mathopen{}\left\{{#1}\right\}\mathclose{}}

<!-- \left\{"STUFF"\right\} -->

\newcommand{\lrs}[1]{\mathopen{}\left[{#1}\right]\mathclose{}}

<!-- \left["STUFF"\right] -->

\newcommand{\lrb}[1]{\mathopen{}\left|{#1}\right|\mathclose{}}

<!-- \left|"STUFF"\right| -->

\newcommand{\inner}[2]{\mathopen{}\left\langle #1, #2 \right\rangle\mathclose{}}

<!-- <\left"STUFF1","STUFF2"\right> -->

\newcommand{\norm}[1]{\mathopen{}\left\lVert#1\right\rVert\mathclose{}}

<!-- \left||"STUFF"\right|| -->

\newcommand{\floor}[1]{\lfloor #1 \rfloor}

<!---Floor function --->

\newcommand{\ceil}[1]{\lceil #1 \rceil}

<!---ceil --->

\newcommand{\FFou}[1]{\mc{F}(#1)}

<!---Fourier Transform notation 1 --->

\newcommand{\Fou}[1]{\widehat{#1}}

<!---Fourier Transform notation 2 --->

<!--- Farver --->

\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}

<!--- Turning text blue --->

\newcommand{\red}[1]{\textcolor{red}{{#1}}}

<!--- Turning text red --->

\newcommand{\green}[1]{\textcolor{green}{{#1}}}

<!--- Turning text green --->

\newcommand{\purple}[1]{\textcolor{purple}{{#1}}}

<!--- Turning text purple --->

\newcommand{\cyan}[1]{\textcolor{cyan}{{#1}}}

<!--- Turning text cyan --->

\newcommand{\orange}[1]{\textcolor{orange}{{#1}}}

<!--- Turning text orange --->

<!--- Oversetting bold accents --->


\newcommand{\boldhat}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\boldbar}[1]{\mathbf{\bar{\text{$#1$}}}}
\newcommand{\boldtilde}[1]{\mathbf{\tilde{\text{$#1$}}}}
\newcommand{\boldcheck}[1]{\mathbf{\check{\text{$#1$}}}}
\newcommand{\indep}{\perp \!\!\! \perp}

<!---independence --->

\newcommand{\colvec}[1]{\begin{pmatrix}{#1}\end{pmatrix}}

<!-- Begin column vector - Doesn't seem to work with non-column vectors...-->

\newcommand{\nd}[2]{\mc{N}\lrp{{#1},{#2}}}

<!-- Normal distribution -->

\newcommand{\dnd}[2]{\sim\mc{N}\lrp{{#1},{#2}}}

<!-- Distributed as Normal distribution -->

\newcommand{\wnd}[3]{\frac{1}{\sqrt{2\pi\cdot {#3}}}e^{-\frac{1}{2}\frac{\lrp{{#1}-{#2}}^2}{{#3}}}}

<!-- With normal density (prob = #1, mean = #2, variance = #3 -->

\newcommand{\wpd}[2]{\frac{{#2}^{{#1}}\cdot e^{-{#2}}}{{#1}\!}}

<!-- With poisson density (prob = #1, mean = #2 -->

\newcommand{\ep}{\varepsilon}

<!-- \newcommand{\Rlogo}{![](R_logo.png){#id .class width=auto height=16px} } <!-- R logo implemented in text -->

<!-- Image insertion alla LaTeX doesn't seem to work too well..., but inserting the above gives the desired effect. -->

<!--???? \declareMathOperator{\SE}{SE} DOESN'T REALLY SEEM TO WORK????-->

<!-- #librar(reshape2) -->

<!-- #librar(lattice) -->

<!-- #librar(hebin) -->

<!-- #librar(xtable) -->

<!-- #librar(splines) -->

<!-- #librar(survival) -->

<!-- #librar(grid) -->

<!-- #librar(lpSolve) -->

<!-- #librar(unit) -->

<!-- #librar(MASS) #NOTE THAT MASS CAN CAUSE CONFLICTS WITH DPLYR OVER SELECT-FUNCTION -->

<!-- For better breaks see https://stackoverflow.com/questions/15622001/how-to-display-only-integer-values-on-an-axis-using-ggplot2 -->
<!-- MatrixHeatMap limits = min and max, or symmetric instead? -->
<!-- Check for positive mindiag argument to SparseMat? -->
<!-- Offdiagonal size options for SparseMat? -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
library(MASS) #Has a conflict with dplyr regarding select.
library(reshape2)
#library(plot.matrix) 
library(Matrix)
library(expm)
#library(plotly) #for 3D plotting
#library(reticulate) #Used for saving plotly images
#library(nloptr) #optimisation
#library(lpSolve) #linear programming
library(microbenchmark)
library(scales) #used for log trans of ggplot
#library(stats4) #mle and stuff
set.seed(314)

varnametotext <- function(v){
   deparse(substitute(v))
}
Stdresplot <- function(model, main = paste("(Estimate, Std. Res.)-plot of", deparse(substitute(model))), ylab ="Standardized residuals", ...) {
 fit <- fitted(model)
 rst <- rstandard(model)
 qplot(fit, rst, main = main, ylab = ylab, ylim = c(-max(3.2,max(abs(rst))), max(3.2,max(abs(rst)))) )+geom_hline(yintercept = 0) #Largest symmetric interval (around 0) of (-3.2,3.2) or (-largest absolute rst, largest absolute rst)
}
QQplotdraw <- function(model, main = paste("Normal QQ-plot of", deparse(substitute(model))), xlab = "Theoretical Quantiles", ylab ="Sample Quantiles", ...) {
   rst <- rstandard(model)
   #dataname <- getCall(lm_LT)$data
   ggplot(data = eval(getCall(model)$data), main = main, xlab = xlab, ylab = ylab) + geom_qq() + geom_qq_line() + aes(sample = rst)
} #main, xlab, ylab call do not work for some reason
StdresQQPlot <- function(model,...) {
   p1 <- Stdresplot(model,...)
   p2 <- QQplotdraw(model,...)
   #library(gridExtra)
   grid.arrange(p1,p2, ncol = 2)
}
```

# 1
We load in the data:
```{r}
movielens <- read.csv("../Data/movielens_top40.csv", header = TRUE)
head(movielens)[,1:6]
```

Some of the movienames are exceptionately long, we shorten some of the longest ones to get more legible dendrograms later:
```{r}
rownames(movielens) <- abbreviate(rownames(movielens), minlength=25)
```

We take a look at the `hclust`  and subsequently the `dist` functions:
```{r}
#?hclust
#?dist
```

We apply `dist` to our data (using default $\ell_2$ distance):
```{r}
movie_dist <- movielens %>% dist()
```

We use `movie_dist` in `hclust` using various clustering methods:
```{r}
movie_clust_single <- movie_dist %>% hclust(method = "single")
movie_clust_average <- movie_dist %>% hclust(method = "average")
movie_clust_complete <- movie_dist %>% hclust(method = "complete")
```

We load in the `ggdendro` package to do dendrograms in a `ggplot`-like enviornment
```{r}
library(ggdendro)
#?ggdendrogram
p_single <- movie_clust_single %>% ggdendrogram(rotate = FALSE) +
   labs(title = "Single Linkage") + 
  theme(plot.title=element_text(hjust=0.5))
p_average <- movie_clust_average %>% ggdendrogram(rotate = F) +
   labs(title = "Average Linkage") +
   theme(plot.title=element_text(hjust=0.5))
p_complete <- movie_clust_complete %>% ggdendrogram(rotate = F) +
   labs(title = "Complete Linkage") +
   theme(plot.title=element_text(hjust=0.5))
p_single
p_average
p_complete
```

We cut off the seperation after four clusters for each of the clustering methods:
```{r}
n_clust <- 4
movie_clust_cut_single <- movie_clust_single %>% cutree(k = n_clust)
movie_clust_cut_average <- movie_clust_average %>% cutree(k = n_clust)
movie_clust_cut_complete <- movie_clust_complete %>% cutree(k = n_clust)
```

We are able to extract the movies in cluster $1$ using (beware of somewhat long output):
```{r}
mccs_df <- movie_clust_cut_single %>% as.data.frame()
mcca_df <- movie_clust_cut_average %>% as.data.frame()
mccc_df <- movie_clust_cut_complete %>% as.data.frame()
colnames(mccs_df) <- c("Cluster")
colnames(mcca_df) <- c("Cluster")
colnames(mccc_df) <- c("Cluster")

(mccs_df_C1 <- mccs_df %>% filter(Cluster == 1))
(mcca_df_C1 <- mcca_df %>% filter(Cluster == 1))
(mccc_df_C1 <- mccc_df %>% filter(Cluster == 1))
```

We may collect the dataframes together using
```{r}
clusterings_df <- cbind(rbind(mccs_df, mcca_df, mccc_df), Method =
               c(rep("Single", nrow(mccs_df)),
                 rep("Average", nrow(mcca_df)),
                 rep("Complete", nrow(mccc_df))))
```


We may find the height at which to cut the tree to seperate into four clusters by for example looking at the dendrogramplots and counting at which point a horizontal line would intercept the dendrogram four times.
```{r}
p_single + geom_hline(yintercept=11.28, colour = "red", linetype = "dashed")
p_average + geom_hline(yintercept=13.55, colour = "red", linetype = "dashed")
p_complete + geom_hline(yintercept=16, colour = "red", linetype = "dashed")
```
Through this method, one can with a little elbowgrees find the $h$ values $h=11.28,13.55,16$ for single, average and complete respectively.


We may investigate whether the clustering clusters based on missing values or by ranking. We may transform the data $0/1$ format, mapping `NA`'s to $0$ and numericals to $1$ by the following commands
```{r}
movielens01 <- movielens
#movielens01[movielens01 %>% is.na()] <- 0; movielens01[movielens01>0] <- 1
movielens01 <- !is.na(movielens)
```

We may calculate the new distance matrix using the $\ell_1$ norm:
```{r}
movie_dist01 <- movielens01 %>% dist(method = "manhattan")
```

We may recalculate the clusterings, cut them, and put them into dataframes
```{r}
movie_clust_single01 <- movie_dist01 %>% hclust(method = "single")
movie_clust_average01 <- movie_dist01 %>% hclust(method = "average")
movie_clust_complete01 <- movie_dist01 %>% hclust(method = "complete")

n_clust <- 4
movie_clust_cut_single01 <- movie_clust_single01 %>% cutree(k = n_clust)
movie_clust_cut_average01 <- movie_clust_average01 %>% cutree(k = n_clust)
movie_clust_cut_complete01 <- movie_clust_complete01 %>% cutree(k = n_clust)

mccs_df01 <- movie_clust_cut_single01 %>% as.data.frame()
mcca_df01 <- movie_clust_cut_average01 %>% as.data.frame()
mccc_df01 <- movie_clust_cut_complete01 %>% as.data.frame()

clusterings_df01 <- cbind(Cluster = rbind(mccs_df01, mcca_df01, mccc_df01), Method =
               c(rep("Single", nrow(mccs_df01)),
                 rep("Average", nrow(mcca_df01)),
                 rep("Complete", nrow(mccc_df01))))
colnames(clusterings_df01) <- c("Cluster", "Method") #For some reason had problems getting 'cluster' colname to stick with cbind
head(clusterings_df01)
```

Finally we could compare the clusterings by analysing the following `comparison_df` dataframe
```{r}
comparison_df <- inner_join(rownames_to_column(clusterings_df), rownames_to_column(clusterings_df01), by = c("rowname" = "rowname", "Method" = "Method"), suffix = c("_NA", "_01"))
head(comparison_df)
comparison_df %>% count(Cluster_NA == Cluster_01)
count(comparison_df, Cluster_NA == Cluster_01)
```
We thus see that in `r count(comparison_df, Cluster_NA == Cluster_01)[1,2]` cases, the different classification approaches disagree, while in the remaining $120-`r count(comparison_df, Cluster_NA == Cluster_01)[1,2]`=`r count(comparison_df, Cluster_NA == Cluster_01)[2,2]`$ cases, the classification agrees. Thus approaches yield different results, and we can conclude that `hclust` does care about more than solely whether or not the values are `NA` for $4$ clusters.


Another approach would be to explore $k$-means methods. Regarding this, we make the new dataset, mapping `NA`'s to zero, but keeping the magnitude of the ratings:
```{r}
movielensNA0 <- movielens; movielensNA0[movielensNA0 %>% is.na()] <- 0
head(movielensNA0)[,1:6]
```

We may cluster the movies into $4$ clusters:
```{r}
n_km_clust <- 4
movie_km <- kmeans(movielensNA0, centers = n_km_clust)
```
We may take a look at the resulting object using `str`
```{r}
str(movie_km)
```

We extract the `size` attribute:
```{r}
movie_km$size
```
Thus the algorithm has placed $`r movie_km[['size']]`$ movies in clusters $1,2,3,4$ respectively.

<!-- !!!Can we do PCA on k-means, just as well as k-means on PCA? - Is there a difference?!!! -->

We might do PCA on the k-means:
```{r}
movie_pca <- prcomp(movielensNA0, scale = TRUE)
str(movie_pca)
movie_km_pca_df <- data.frame(movie_pca$x, movie_km$cluster)
head(movie_km_pca_df)[,c(1,2,ncol(movie_km_pca_df))]
```
Calculate the "Variance explained" for the two first principle components
```{r}
PCVarEx <- as.numeric(movie_pca$sdev^2)[1:2]/sum(as.numeric(movie_pca$sdev^2))
```
And subsequently plot the PCA plot:
```{r}
ggplot(movie_km_pca_df, 
       aes(x = PC1, y = PC2, colour = as.factor(movie_km.cluster))) +
   geom_point(size = 2) +  labs(colour = 'Cluster', 
                               x = paste0("PC1 (", signif(PCVarEx[1],3)*100, "%)"),
                               y = paste0("PC2 (", signif(PCVarEx[2],3)*100, "%)"),
                               title = paste0("movielensNA0 PCA plot of k-means, k = ",                                 n_km_clust)) 

```

<!-- Note that we are "cheating" below -->

<!-- ```{r} -->
<!-- #library(ggfortify) -->
<!-- #autoplot(movie_km, data = movielensNA0) -->
<!-- ``` -->
<!--??? R-mode PCA vs. Q-mode PCA ???-->

<!-- ```{r} -->
<!-- x <- rbind(matrix(rnorm(2000, sd = 123), ncol = 2), -->
<!--            matrix(rnorm(2000, mean = 800, sd = 123), ncol = 2)) -->
<!-- colnames(x) <- c("x", "y") -->
<!-- x <- data.frame(x) -->

<!-- A <- kmeans(x, centers = 3, nstart = 50, iter.max = 500) -->
<!-- #princomp$scores = prcomp$x (ish(?)) -->
<!-- #pca_x <- princomp(x) -->
<!-- pca_x_2 <- prcomp(x, scale = T) -->
<!-- str(pca_x_2) -->
<!-- #pca_x$scores -->
<!-- x_cluster = data.frame(pca_x_2$x,A$cluster) -->
<!-- x_cluster -->
<!-- ggplot(x_cluster, aes(x = PC1, y = PC2, color = as.factor(A.cluster), fill = as.factor(A.cluster))) + geom_point() + -->
<!--   stat_ellipse(type = "t",geom = "polygon",alpha = 0.4) -->
<!-- ``` -->

We may calculate k-means for $k$ from $2$ to $6$ and compare the internal and external cluster distances resulting therefrom.

<!-- Why does the rbind not work? -->
<!-- # movie_km_2t6var_df <- data.frame(variation = rbind(sapply(1:5, function(x) { -->
<!-- #    movie_km_2t6[[x]]$tot.withinss}), sapply(1:5, function(x) { -->
<!-- #    movie_km_2t6[[x]]$betweenss})), type = c(rep("Within", 5), rep("Between",5), k = rep(2:6,2))) -->
```{r}
kvals <- 2:6
movie_km_2t6 <- lapply(kvals, function(k){
    kmeans(movielensNA0, centers = k)
})

movie_km_2t6var_df <- melt(data.frame(Within = sapply(kvals-1, function(x) {
   movie_km_2t6[[x]]$tot.withinss}), Between = sapply(kvals-1, function(x) {
   movie_km_2t6[[x]]$betweenss}), k = kvals), id.vars = "k")
movie_km_2t6var_df

ggplot(movie_km_2t6var_df, aes(x = k, y = value, colour = variable)) + geom_point() + geom_line() + labs(title = paste0("Total SS-distances within and between clusters for varying k"), y = "Sum of Squares", colour = 'Distance type')
```
A 'bend' seems to appear at $k=3$ for both the within and betweenness. One could therefore investigate further, whether $k=3$ would be a pertinent choice.

#2
We may load in the data:
```{r}
author.dat <- read.csv("../Data/author_count.csv", header = TRUE)
numeric.dat <- author.dat[-1]
authors <- factor(author.dat[[1]])
head(author.dat)[,1:6]
```

And calculate the PCA, incorporating scaling and centering:
```{r}
word_pca <- prcomp(numeric.dat, scale = T, center = T)
word_pca_wlabel <- data.frame(word_pca$x, authors)
#head(word_pca_wlabel)[,1:6]
word_pcVarEx <- as.numeric(word_pca$sdev^2)[1:2]/sum(as.numeric(word_pca$sdev^2))
```
and plot the first two principle components, also calculating the proportion of variance of each component:
```{r, warning=FALSE, message=FALSE}
(word_pca_p <- ggplot(word_pca_wlabel, 
       aes(x = PC1, y = PC2, colour = as.factor(word_pca_wlabel$authors))) +
   geom_point(size = 2, alpha = 0.8) +  labs(colour = 'Author', 
                               x = paste0("PC1 (", signif(word_pcVarEx[1],3)*100, "%)"),
                               y = paste0("PC2 (", signif(word_pcVarEx[2],3)*100, "%)"),
                               title = paste0("PCA plot of word occurrences")))
```
We note that while Austen and London seem to have rather good seperation, $2$ dimensional PCA has not been able to seperate out all of the Milton data from the Shakespeare.

We may attempt to provide a better visualisation of the seperation using tSNE.
```{r, warning = F}
#install.packages("Rtsne")
library(Rtsne)
```
Looking into `Rtsne` we see quite a number of parameters that are possible to tune. We are choosing only to tune the `perplexity` and leaving everything else as-is. In choosing which perplexity levels are worth investigating, we try levels of multiples of $5$ from $5$ up to the number of features $69,$ and add $1$ to get $14$ plots in total. 
```{r}
#?Rtsne #checking doc
perPlexVals <- seq(5,dim(numeric.dat)[2]+1,5)
tsne_df_list <- lapply(perPlexVals, function(p) {
    data.frame(dim = Rtsne(numeric.dat, perplexity = p, max_iter = 1000)[['Y']], labels = authors)})
# why does which(p == ...) not work?
plots <- lapply(perPlexVals, function(p) {
   ggplot(tsne_df_list[[p/5]], aes(dim.1, dim.2, colour = labels)) + geom_point() + ggtitle(paste0("Word occurrence tSNE. Perplexity = ", p)) + labs(x = "Dimension 1", y  = "Dimension 2", colour = 'Author')
})
#do.call("grid.arrange", c(plots, ncol = 2))
# for (i in seq(1,13,2)) {
#    grid.arrange(plots[[i]], plots[[i+1]], ncol = 2)
# }
grid.arrange(plots[[1]], plots[[2]], ncol = 1)
grid.arrange(plots[[3]], plots[[4]], ncol = 1)
grid.arrange(plots[[5]], plots[[6]], ncol = 1)
grid.arrange(plots[[7]], plots[[8]], ncol = 1)
grid.arrange(plots[[9]], plots[[10]], ncol = 1)
grid.arrange(plots[[11]], plots[[12]], ncol = 1)
grid.arrange(plots[[13]], plots[[14]], ncol = 1)
```
As all perplexity levels do a very good job at isolating the clusters of the authors (noting that we have employed a default random seed of $314$), we may also try with perplexity $=3$ to see if there is a difference:
```{r}
data.frame(dim = Rtsne(numeric.dat, perplexity = 3, max_iter = 1000)[['Y']], labels = authors) %>% ggplot(aes(dim.1, dim.2, colour = labels)) + geom_point() + ggtitle(paste0("Word occurrence tSNE. Perplexity = ", 3)) + labs(x = "Dimension 1", y = "Dimension 2", colour = 'Author')
```
The tSNE for this lower perplexity value is once again very good at isolating the points, it does however become more evident how the lower perplexity is tuning SNE to prefer local clusterings.

We may also try to plot the data using MDS. In aid of this, we calculate MDS based on the metrics of the Euclidean, the maximum and the Manhattan. We will use the default `cmdscale` $k$ of $2$ as we want to do $2D$ plots.
```{r}
mds_euc <- data.frame(dim = cmdscale(dist(numeric.dat, method = "euclidean")), author = authors)

mds_max <- data.frame(dim = cmdscale(dist(numeric.dat, method = "maximum")), author = authors)

mds_man <- data.frame(dim = cmdscale(dist(numeric.dat, method = "manhattan")), author = authors)
```

We may plot the three results:
```{r, warning = F}
mds_euc %>% ggplot(aes(dim.1, dim.2, colour = author)) +
   geom_point(size = 2, alpha = 0.7) + labs(x = "Dimension 1",
                               y = "Dimension 2",
                               colour = 'Author',
                               title = "Word occurrence Euclidean MDS")

mds_max %>% ggplot(aes(dim.1, dim.2, colour = author)) +
   geom_point(size = 2, alpha = 0.7) + labs(x = "Dimension 1",
                               y = "Dimension 2",
                               colour = 'Author',
                               title = "Word occurrence Maximum MDS")

(mds_man_p <- mds_man %>% ggplot(aes(dim.1, dim.2, colour = author)) +
   geom_point(size = 2, alpha = 0.7) + labs(x = "Dimension 1",
                               y = "Dimension 2",
                               colour = 'Author',
                               title = "Word occurrence Manhattan MDS"))
```

We see that while the Euclidean and Manhattan distances do a reasonable job at separating the points, MDS struggles to separate out the authors based on the maximum norm.

Comparing the different methods, we may assess a best implementation of each and compare these. Having only one implementation of PCA we pick that one. With all the tSNE plots being rather good at splitting the works of the four authors apart, we choose to work based on perplexity  $=10.$ While not perfect, we assess that the $\ell_1$ MDS does the best job of separating the works of the authors.
We may replot the three plots to remind ourselves of them:
```{r, warning=FALSE}
word_pca_p
plots[[2]]
mds_man_p
```


We see from the plots, that while tSNE does an excellent job seperating out the author clusters with only a very small number of outliers, the Manhattan MDS and PCA seem to have the same struggle of separating out the word-frequency of use of Milton's works, from those of Shakespeare, with the Manhattan-based MDS also having some overlap between London and Milton. 