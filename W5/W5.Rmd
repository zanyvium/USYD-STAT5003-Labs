---
title: "W5 STAT5003 RMD"
author: "Victor Z. Nygaard, vnyg7406"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{verbatim}
   - \usepackage[language]{babel}
   - \usepackage[encoding]{inputenc}
   - \usepackage{hyperref}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \usepackage{amssymb}
   - \usepackage{mathtools}
   - \usepackage{nicefrac}
   - \usepackage{fullpage}
   - \usepackage{stmaryrd}
   - \usepackage{aligned-overset}
   - \usepackage{pdfpages}
date: "Last compiled on `r format(Sys.time(), '%d. %B, %Y')`"
# Hvordan får  vi den til at printe February i stedet for februar??????
output: html_document
---

------------------------------------------------------------------------

```{=html}
<!-- RMD tips:
1. CTRL+SHIFT+C RMD-comments-out the selected lines, with in a standard HTML comment-out format. 

2. CTRL+ALT+I inserts a new r codechunck

3. Pressing CTRL+SHIFT+ENTER when over a code chunck gives you a preview of the results of the chunck

4. CTRL+SHIFT+K gives you a preview of the entire resulting HTML file.

5. Note the different results of '#HS 1' and 
'# HS 1' (without the '') in the output

6. It is possible to compile regular R-scripts into Rmd files - this is done by pressing CTRL+SHIFT+K while attending any R-script. <<<- Though this apparently doesn't work for HS Problems.R for some reason!?!?!? ->>>

7. Selective use of the echo=c(...) option within code chuncks allows assignment of a variable, to show the assignment in the knitted document, and showing the value of the assignment seamlessly as well - see HS2.3

8. It is possible to include results of R-analysis such as summary statistics in LaTeX-equations in RMD, see HS2.3

9. Adding fig.align="center" to a code chunk centers any figures generated by the chunck.

9.1 note that properties of codechunks seem casesensitive; fig.align="center" centers a figure, but fig.align="Center" (with capital C) doesn't

10. A new subtitle needs a blank line before itself: 
'works:

#### HS 7
'

'doesn't:
blablabla
#### HS 7
'

'doesn't either:
<!-- blablabla ->
#### HS 7
'
11. Pressing F7 when marking, or hovering over a word will spellcheck the word

12. CTRL + - (minus) zooms out, CTRL + + (plus) zooms in

13. CTRL + D Deletes the current line, or current selection of lines

14. THE FOLLOWING SOURCE EDITOR FOLDING METHODS:
14.1 Collapse current fold: ALT + L
14.1.1: Expand current fold: SHIFT + ALT + L
14.2 Collapse "all" subfolds: ALT + O <- !?!! Note that this leaves a small letter 'o' in the text !!?!
14.2.1 Expand "all" subfolds: SHIFT + ALT + O
14.3 Collapse all other folds: ALT + 0 (zero)

15. SHIFT + ALT + J allows you to jump to specific parts of the document

16. Writing a new line with '...' will cause all previous output to be hidden in the knittet document

17. Writing (q<-5) around R code, will both assign and print the code upon assignment 

18. Note that 'attach' only has the scope of the current R-chunck.

19. One way to get pdf printout is to compile a html-printout, and then, in-browser, 'print' the HTML page as a pdf.

20. CTRL + SHIFT + M gives the pipe operator.

21. Pressing CTRL + F3 searches on the selected word.

22. CTRL

23. ALT + SHIFT + DOWN copies a line to below.
-->
```
<!-- ---?--- How do I create a closeable Rmd section, such that I do not have to scroll through the LaTeX commands each time? - !!! Can be done with '-----' through this also creates a line in the knittet document. -->

<!-- How do I publish and share the HTML as a viewable (and linkable) website - this can be done through github? -->

<!-- How can I share R markdown files such that multiple people can edit them at the same time? -->

<!-- Do we need parindent controls as in LaTeX? -->

<!-- Use of the cache function to reduce recompile times -->

<!-- How to close current subsection with a keyboard shortcut? How to close subsubsections,...? - !!!See RMD tip 14!!! -->

<!-- Chunk naming? -->

<!-- How to define variables such that they have scope within their own ## segment? -->

<!-- How do I delete all non-needed variables for each new section in R??? -->

<!-- LaTeX commands -->

\newcommand{\C}{\mathbb{C}}

<!--- Komplekse tal --->

\newcommand{\R}{\mathbb{R}}

<!--- Reelle tal--->

\newcommand{\Q}{\mathbb{Q}}

<!---Rationelle tal--->

\newcommand{\Z}{\mathbb{Z}}

<!---Hele tal--->

\newcommand{\N}{\mathbb{N}}

<!---Naturlige tal--->

\newcommand{\E}{\mathbb{E}}

<!---mean--->

\newcommand{\F}{\mathbb{F}}

<!---Baggrundsrum sigma-alg--->

\newcommand{\B}{\mathbb{B}}

<!---Borel sigma--->

\newcommand{\K}{\mathbb{K}}

<!---Generel field--->

\newcommand{\RB}{\overline{\R}}

<!---Udvidede reelle tal--->

\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\BR}{\mathcal{B}\left(\R\right)}

<!---Borel på Reelle tal -->

\newcommand{\BRB}{\mathcal{B}\left(\RB\right))}

<!---Borel på udvidede reelle tal -->


\newcommand{\mf}[1]{\mathfrak{#1}} 
\newcommand{\mcG}[2]{\mathcal{#1}^1(#2)} 
\newcommand{\mcGG}[4]{\mathcal{#1}_{#3}^{#2}(#4)}
\newcommand{\GMR}{\left(X,\ms{A},\mu\right)}

<!---Generelt målrum -->

\newcommand{\PBS}{\lrp{\Omega,\F, P}}

<!--- Probability background space -->

\newcommand{\RMR}{\left(\R,\BR, \lambda\right)}

<!---Reelt målrum, m. Borel, og lebesgue mål. -->

<!---L_p spaces on [0,1] with m -->


\newcommand{\Lp}[1]{L_{#1}\lrp{\lrs{0,1},m}} 
\newcommand{\mclxy}{\mc{L}\lrp{X,Y}}
<!---Bounded linear functionals from X to Y -->

\newcommand{\mckxy}{\mc{K}\lrp{X,Y}}

<!---Compact Bounded linear functionals from X to Y -->

\newcommand{\mssr}{\ms{S}(\R)}

<!---The Schwartz space on $\R$ -->

<!---Arrows -->

\newcommand{\ra}{\rightarrow}

<!---Konvergens pil højre -->

\newcommand{\nra}{\nrightarrow}

<!---ikke Konvergens pil højre -->

\newcommand{\la}{\leftarrow}

<!---Konv pil venstre -->

\newcommand{\nla}{\nleftarrow}

<!---ikke Konvergens pil venstre -->

\newcommand{\lra}{\leftrightarrow}

<!---højre venstre pil -->

\newcommand{\nlra}{\nleftrightarrow}

<!---ikke højre venstre pil -->

\newcommand{\hra}{\hookrightarrow}

<!---Injektiv  pil højre -->

\newcommand{\Ra}{\Rightarrow}

<!---Implikations pil højre -->

\newcommand{\Lra}{\Leftrightarrow}

<!---Bi-implikations pil -->

\newcommand{\Uda}{\Updownarrow}

<!---Bi-implikations pil (op og ned) -->

\newcommand{\Da}{\Downarrow}

<!---implikations pil (ned) -->

\newcommand{\rhpu}{\rightharpoonup}

<!---Weak convergence in Hilbert spaces -->

<!-- LHS & RHS calculations -->

\newcommand{\swel}{\overset{\swarrow}{=}}

<!---Continue calculation on left hand side with equality -->

\newcommand{\sweq}{\overset{\swarrow}{\equiv}}

<!---Continue calculation on left hand side with equivalence -->

\newcommand{\seel}{\overset{\searrow}{=}}

<!---Continue calculation on right hand side with equality -->

\newcommand{\seeq}{\overset{\searrow}{\equiv}}

<!---Continue calculation on right hand side with equivalence -->

\newcommand{\inse}{\overset{\cdot}{=}}

<!--- Insert values in calculation -->

\newcommand{\eqd}{\overset{d.}{=}}

\newcommand{\PMX}{\mc{P}\left(X\right)}

<!---Potensmængde af X -->

\newcommand{\comp}{\mathsf{c}}

<!---Set compliment -->

\newcommand{\sm}{\setminus}

<!---mængdedifferens -->

<!--- Parenteser --->

\newcommand{\lrp}[1]{\mathopen{}\left({#1}\right)\mathclose{}}

<!-- \left("STUFF"\right) -->

\newcommand{\lrc}[1]{\mathopen{}\left\{{#1}\right\}\mathclose{}}

<!-- \left\{"STUFF"\right\} -->

\newcommand{\lrs}[1]{\mathopen{}\left[{#1}\right]\mathclose{}}

<!-- \left["STUFF"\right] -->

\newcommand{\lrb}[1]{\mathopen{}\left|{#1}\right|\mathclose{}}

<!-- \left|"STUFF"\right| -->

\newcommand{\inner}[2]{\mathopen{}\left\langle #1, #2 \right\rangle\mathclose{}}

<!-- <\left"STUFF1","STUFF2"\right> -->

\newcommand{\norm}[1]{\mathopen{}\left\lVert#1\right\rVert\mathclose{}}

<!-- \left||"STUFF"\right|| -->

\newcommand{\floor}[1]{\lfloor #1 \rfloor}

<!---Floor function --->

\newcommand{\ceil}[1]{\lceil #1 \rceil}

<!---ceil --->

\newcommand{\FFou}[1]{\mc{F}(#1)}

<!---Fourier Transform notation 1 --->

\newcommand{\Fou}[1]{\widehat{#1}}

<!---Fourier Transform notation 2 --->

<!--- Farver --->

\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}

<!--- Turning text blue --->

\newcommand{\red}[1]{\textcolor{red}{{#1}}}

<!--- Turning text red --->

\newcommand{\green}[1]{\textcolor{green}{{#1}}}

<!--- Turning text green --->

\newcommand{\purple}[1]{\textcolor{purple}{{#1}}}

<!--- Turning text purple --->

\newcommand{\cyan}[1]{\textcolor{cyan}{{#1}}}

<!--- Turning text cyan --->

\newcommand{\orange}[1]{\textcolor{orange}{{#1}}}

<!--- Turning text orange --->

<!--- Oversetting bold accents --->


\newcommand{\boldhat}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\boldbar}[1]{\mathbf{\bar{\text{$#1$}}}}
\newcommand{\boldtilde}[1]{\mathbf{\tilde{\text{$#1$}}}}
\newcommand{\boldcheck}[1]{\mathbf{\check{\text{$#1$}}}}
\newcommand{\indep}{\perp \!\!\! \perp}

<!---independence --->

\newcommand{\colvec}[1]{\begin{pmatrix}{#1}\end{pmatrix}}

<!-- Begin column vector - Doesn't seem to work with non-column vectors...-->

\newcommand{\nd}[2]{\mc{N}\lrp{{#1},{#2}}}

<!-- Normal distribution -->

\newcommand{\dnd}[2]{\sim\mc{N}\lrp{{#1},{#2}}}

<!-- Distributed as Normal distribution -->

\newcommand{\wnd}[3]{\frac{1}{\sqrt{2\pi\cdot {#3}}}e^{-\frac{1}{2}\frac{\lrp{{#1}-{#2}}^2}{{#3}}}}

<!-- With normal density (prob = #1, mean = #2, variance = #3 -->

\newcommand{\wpd}[2]{\frac{{#2}^{{#1}}\cdot e^{-{#2}}}{{#1}\!}}

<!-- With poisson density (prob = #1, mean = #2 -->

\newcommand{\ep}{\varepsilon}

<!-- \newcommand{\Rlogo}{![](R_logo.png){#id .class width=auto height=16px} } <!-- R logo implemented in text -->

<!-- Image insertion alla LaTeX doesn't seem to work too well..., but inserting the above gives the desired effect. -->

<!--???? \declareMathOperator{\SE}{SE} DOESN'T REALLY SEEM TO WORK????-->

<!-- #librar(reshape2) -->

<!-- #librar(lattice) -->

<!-- #librar(hebin) -->

<!-- #librar(xtable) -->

<!-- #librar(splines) -->

<!-- #librar(survival) -->

<!-- #librar(grid) -->

<!-- #librar(lpSolve) -->

<!-- #librar(unit) -->

<!-- #librar(MASS) #NOTE THAT MASS CAN CAUSE CONFLICTS WITH DPLYR OVER SELECT-FUNCTION -->




<!-- # !!!dplyr::select(indNoNA, where(is.numeric)) -->
<!-- indNoNA <- indians %>% drop_na() %>% mutate(across(c(pregnant:insulin,age), as.integer)) -->
<!-- install.packages("ggrepel") -->
<!-- !!! ```{r} -->
<!-- gdf <- -->
<!--   tibble(g = c(1, 1, 2, 3), v1 = 10:13, v2 = 20:23) %>% -->
<!--   group_by(g) -->
<!-- gdf -->

<!-- set.seed(1) -->

<!-- # Outside: 1 normal variate -->
<!-- n <- rnorm(1) -->
<!-- n -->
<!-- gdf %>% mutate(across(v1:v2, ~ .x + n)) -->
<!-- ``` -->

<!-- !!! ```{r} -->
<!-- ? family -->
<!-- ``` -->

<!-- !!! # ```{r} -->
<!-- # ifelse -->
<!-- # ``` -->

<!-- # ```{r} -->
<!-- # df <- df %>% -->
<!-- #   mutate(n = row_number()) %>% #create row number if you dont have one -->
<!-- #   select(n, everything()) # put 'n' at the front of the dataset -->
<!-- # train <- df %>% -->
<!-- #   group_by(var1, var2) %>% #any number of variables you wish to partition by proportionally -->
<!-- #   sample_frac(.7) # '.7' is the proportion of the original df you wish to sample -->
<!-- # test <- anti_join(df, train) # creates test dataframe with those observations not in 'train.' -->
<!-- # ``` -->


<!-- ```{r} -->
<!-- RColorBrewer::brewer.pal(10, "Set1") #but then try with 9 -->
<!-- ``` -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
library(MASS) #Has a conflict with dplyr regarding select.
library(reshape2)
library(caret) #Used for Machine Learning
#library(plot.matrix) 
library(Matrix)
library(expm)
#library(plotly) #for 3D plotting
#library(reticulate) #Used for saving plotly images
#library(nloptr) #optimisation
#library(lpSolve) #linear programming
library(microbenchmark)
library(scales) #used for log trans of ggplot
#library(stats4) #mle and stuff
set.seed(314)

varnametotext <- function(v){
   deparse(substitute(v))
}
Stdresplot <- function(model, main = paste("(Estimate, Std. Res.)-plot of", deparse(substitute(model))), ylab ="Standardized residuals", ...) {
 fit <- fitted(model)
 rst <- rstandard(model)
 qplot(fit, rst, main = main, ylab = ylab, ylim = c(-max(3.2,max(abs(rst))), max(3.2,max(abs(rst)))) )+geom_hline(yintercept = 0) #Largest symmetric interval (around 0) of (-3.2,3.2) or (-largest absolute rst, largest absolute rst)
}
QQplotdraw <- function(model, main = paste("Normal QQ-plot of", deparse(substitute(model))), xlab = "Theoretical Quantiles", ylab ="Sample Quantiles", ...) {
   rst <- rstandard(model)
   #dataname <- getCall(lm_LT)$data
   ggplot(data = eval(getCall(model)$data), main = main, xlab = xlab, ylab = ylab) + geom_qq() + geom_qq_line() + aes(sample = rst)
} #main, xlab, ylab call do not work for some reason
StdresQQPlot <- function(model,...) {
   p1 <- Stdresplot(model,...)
   p2 <- QQplotdraw(model,...)
   #library(gridExtra)
   grid.arrange(p1,p2, ncol = 2)
}
```


# 1.
We load in the required packages and data:
```{r, warning = F}
data(PimaIndiansDiabetes2, package = "mlbench")
library(caret)
library(e1071) #SVMs
indians <- PimaIndiansDiabetes2
rm(PimaIndiansDiabetes2)
```
We inspect the data using `head`, `str`, `glimpse` and `summary`
```{r}
head(indians)
str(indians)
glimpse(indians)
summary(indians)
```


We may remove rows containing `NA`s and correct the datatypes of `pregnant` and `age` to be integer,  
```{r}
#indNA <- indians %>% subset(apply(indians, 1, anyNA))
#indNoNA2 <- indians %>% anti_join(indNA) %>% mutate(across(c(pregnant, age), as.integer))
indNoNA <- indians %>% drop_na() %>% mutate(across(c(pregnant, age), as.integer))
#identical(indNoNA2, indNoNA)
```

and plot the pairwise relations between the variables using `GGally`s `ggpairs`, noting that we add a small jitter to the point plots, in order to have less overlap:
```{r, message = F, }
library(GGally) #For pairwise plots
(ggp <- indNoNA %>% ggpairs(mapping = aes(alpha = 0.4, colour = diabetes), progress = F, columnLabels = abbreviate(colnames(indNoNA), 6),
                            upper = list(continuous = wrap("cor", size = 2)),
                            lower = list(continuous = wrap("points", size = 0.65, alpha = 0.35, position = position_jitter(w = 0.1, h = 0.15)))) + 
      scale_colour_brewer(palette = "Set1")) +
   theme(axis.text.x = element_text(size = 5.5),
         axis.text.y = element_text(size = 5))
```
We note that the plot suggests colinearity amongst the NA-dropped data between `mass` and `triceps`, `insulin` and `glucose`, and between `pregnant` and `age`, of which the last two are immediately interpretable. Having grouped and coloured the data by whether they are positive for diabetes, is is even estimated that the correlation between glucose and insulin is signifigantly lower for positive test-subjects, than for negative test-subjects. We also have the immediately interpretable of `pregnant`-`age` correlation, as older women have had more time to have more kids. We may ponder the reasons for the calculated correlations amongst the other feature-pairs based on domain knowledge and looking through `?mlbench::PimaIndiansDiabetes2`.

# 2.
We implement the logistic regression for `diabetes` response, including all other features as predictors, and including these additively. We might note that by the above glimpses into the data, the first level of `diabetes` is `neg`, and thus the `success` probabilities will be in relation to 'not `neg`' which in our case is just `pos`, noting also that the canonically implemented link-function for `family = "binomial"` is the logit, by `? family`
```{r}
logmodel <- glm(diabetes ~ ., family = "binomial", data = indNoNA)
summary(logmodel)
```
<!-- Diagnostic plots -->
<!-- Plotting of the sigmoid curve -->
We see from the summary that both the intercept and `glucose` are calculated to be strongly significant, with `mass` and `pedigree` being significant up to atleast a $\alpha = 0.01$ level. From the summary, we may also read off that four largest estimates (noting also that logit is increasing) were
```{r}
sort(summary(logmodel)$coef[,1], decreasing = T)[1:4]
```

though both the significance levels and the precise level of the estimates can be critised, under the consideration of the implications of having simply removed missing data, and considering possible colinearity. 
We may also transform the estimates to '$p$-level', using the inverse of the logit function
```{r}
logitInv <- function(x) {exp(x)/(1+exp(x))}
logitInv(summary(logmodel)$coef[,1])
```


We may evaluate the accuracy of the model on the training data, based on the $X\beta>0 = p\geq 0.5$ criteria as
```{r}
log_pred_class <- ifelse(predict(logmodel)>0, "pos", "neg") #predicting the class
mean(log_pred_class == indNoNA$diabetes) #the accuracy
```

# 3.
With the `caret` package loaded, we may split the data,
<!-- Stratification? -->

<!-- # ```{r} -->
<!-- # df <- data.frame(a = rep("a",100), b = 1:100) -->
<!-- # df -->
<!-- # df <- df %>% -->
<!-- #   mutate(n = row_number()) %>% #create row number if you dont have one -->
<!-- #   dplyr::select(n, everything()) # put 'n' at the front of the dataset -->
<!-- # df -->
<!-- # train <- df %>% -->
<!-- #   dplyr::group_by(b) %>% #any number of variables you wish to partition by proportionally -->
<!-- #   dplyr::slice_sample(prop = 0.75) # '.75' is the proportion of the original df you wish to sample -->
<!-- # train -->
<!-- # test <- anti_join(df, train) # creates test dataframe with those observations not in 'train.' -->
<!-- # ?createDataPartition() -->
<!-- # ``` -->


```{r, message = F}
indTrainIndex <- createDataPartition(indNoNA$diabetes, p = 0.75, times = 1, list = F)
indTrain <- indNoNA[indTrainIndex,]
indTest <- indNoNA[-indTrainIndex,]
```


An alternative way of doing the splitting utilising `tidyverse` is
```{r}
indTrainTidy <- indNoNA %>%
  dplyr::group_by(diabetes) %>% #any number of variables you wish to partition by proportionally
  dplyr::slice_sample(prop = 0.75) # '.75' is the proportion of the original df you wish to sample
indTestTidy <- anti_join(indNoNA, indTrainTidy) # creates test dataframe with those observations not in 'train.'
```

with the data seperated, we may use `caret` to train each of logistic regression, LDA, kNN and SVM across the training dataset, on the additive form.

```{r}
logTrainModel <- train(diabetes ~ .,
                     data = indTrain,
                     method = "glm", family = "binomial",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
ldaTrainModel <- train(diabetes ~ .,
                     data = indTrain,
                     method = "lda",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
knnTrainModel <- train(diabetes ~ .,
                     data = indTrain,
                     method = "knn",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
svmTrainModel <- train(diabetes ~ .,
                     data = indTrain,
                     method = "svmLinearWeights",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
```

<!-- ```{r} -->
<!-- logTrainModelTidy <- train(diabetes ~ ., -->
<!--                      data = indTrainTidy, -->
<!--                      method = "glm", family = "binomial", -->
<!--                      trControl = trainControl(method = "repeatedcv", -->
<!--                                               repeats = 5)) -->
<!-- ldaTrainModelTidy <- train(diabetes ~ ., -->
<!--                      data = indTrainTidy, -->
<!--                      method = "lda", -->
<!--                      trControl = trainControl(method = "repeatedcv", -->
<!--                                               repeats = 5)) -->
<!-- knnTrainModelTidy <- train(diabetes ~ ., -->
<!--                      data = indTrainTidy, -->
<!--                      method = "knn", -->
<!--                      trControl = trainControl(method = "repeatedcv", -->
<!--                                               repeats = 5)) -->
<!-- svmTrainModelTidy <- train(diabetes ~ ., -->
<!--                      data = indTrainTidy, -->
<!--                      method = "svmLinearWeights", -->
<!--                      trControl = trainControl(method = "repeatedcv", -->
<!--                                               repeats = 5)) -->
<!-- ``` -->


We may take a look at the returned objects and a summary of each of these:
```{r}
logTrainModel
summary(logTrainModel)

ldaTrainModel
summary(ldaTrainModel)

knnTrainModel
summary(knnTrainModel)

svmTrainModel
summary(svmTrainModel)
```


Having trained the models, we may use them to predict the diabetes status of the subjects in the training data.
```{r}
pred_logTrainModel <- predict(logTrainModel, newdata = indTest)
pred_ldaTrainModel <- predict(ldaTrainModel, newdata = indTest)
pred_knnTrainModel <- predict(knnTrainModel, newdata = indTest)
pred_svmTrainModel <- predict(svmTrainModel, newdata = indTest)
```


We may evaluate the accuracy of these models
```{r}
mean(pred_logTrainModel == indTest$diabetes) #log
mean(pred_ldaTrainModel == indTest$diabetes) #LDA
mean(pred_knnTrainModel == indTest$diabetes) #kNN
mean(pred_svmTrainModel == indTest$diabetes) #SVM
```

Having calculated the test set accuracies above we note that the accuracies are rather similar with a standard deviation of just `r sd(c(mean(pred_logTrainModel == indTest$diabetes),
mean(pred_ldaTrainModel == indTest$diabetes),
mean(pred_knnTrainModel == indTest$diabetes),
mean(pred_svmTrainModel == indTest$diabetes)))`

We may do a confusion matrix overview of each of the models, and compare these:
```{r}
#?caret::confusionMatrix
caret::confusionMatrix(data = pred_logTrainModel, reference = indTest$diabetes, positive = "pos") #log
caret::confusionMatrix(data = pred_ldaTrainModel, reference = indTest$diabetes, positive = "pos") #LDA
caret::confusionMatrix(data = pred_knnTrainModel, reference = indTest$diabetes, positive = "pos") #kNN
caret::confusionMatrix(data = pred_svmTrainModel, reference = indTest$diabetes, positive = "pos") #SVM
```


<!-- Positive class: neg? Wasnt it pos? -->
For which we see once again very similar results regarding sensitivity and specificity.

# 4. 

We redo the logistic, kNN, and SVM models, now featuring only `mass` and `glucose` as additive predictors. We do this across the entire `NA`-omitted data `indNoNA` for illustration. Note that we use `method = svmRadial` for employing the radial kernel in the SVM model.
```{r, message = F, warning = F}
logSmallModel <- train(diabetes ~ mass + glucose,
                     data = indNoNA,
                     method = "glm", family = "binomial",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
knnSmallModel <- train(diabetes ~ mass + glucose,
                     data = indNoNA,
                     method = "knn",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
svmSmallModel_lin <- train(diabetes ~ mass + glucose,
                     data = indNoNA,
                     method = "svmLinearWeights",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))

library(kernlab)
svmSmallModel_rad <- train(diabetes ~ mass + glucose,
                     data = indNoNA,
                     method = "svmRadial",
                     trControl = trainControl(method = "repeatedcv",
                                              repeats = 5))
```

We may replot the plot of `mass` and `glucose` from the `ggpairs` plot, now in large format:
```{r}
(pMassGlu <- indNoNA %>% ggplot(mapping = aes(x = mass, y = glucose, colour = diabetes)) + geom_point(alpha = 0.7) + scale_colour_discrete(labels=c('Negative', 'Positive')) + labs(title = "Mass-Glucose plot"))
```
<!--??? # + scale_colour_brewer(palette = "Set1") Combining adjusted legend and palette change -->

We may additionally calculate the decision boundaries, for logistic regression and the linear SVM and plot these (remembering to extract the final model from `caret::train` using `$finalModel`)
<!-- # WHY DO SVM LINES NOT SHOW UP?! -->
```{r}
#the coefficient estimates of logistic model
log_s_logit_coef <- as.numeric(summary(logSmallModel)$coef[,1])
(yint_log <- -log_s_logit_coef[1]/log_s_logit_coef[3])
(slo_log <- -log_s_logit_coef[2]/log_s_logit_coef[3])

#svm parameters
(w <- t((svmSmallModel_lin$finalModel)$coefs) %*% (svmSmallModel_lin$finalModel)$SV)
beta_0_svm <- (svmSmallModel_lin$finalModel)$rho


pMassGlu + 
   geom_abline(intercept = yint_log, slope = slo_log, size = 1.2,
               linetype = "dotted", colour = "black") +
   geom_abline(intercept = (-beta_0_svm-(svmSmallModel_lin$finalModel)$cost) / w[1, 2], slope = -w[1, 1] / w[1, 2], size = 5, linetype = "dashed", colour = "green") +
   geom_abline(intercept = (-beta_0_svm+(svmSmallModel_lin$finalModel)$cost) / w[1, 2], slope = -w[1, 1] / w[1, 2], size = 5, linetype = "dotted", colour = "black") +
   labs(title = "MASS-Glucose SVM with margins, logistic decision boundary")
```


















