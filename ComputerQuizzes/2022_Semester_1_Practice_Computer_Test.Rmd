---
title: "Computer Test"
subtitle: "STAT5003 Semester 2, 2022"
date: "14th september, 2022"
author: "Victor 520540446 - TEST STUDENT"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{verbatim}
   - \usepackage[language]{babel}
   - \usepackage[encoding]{inputenc}
   - \usepackage{hyperref}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \usepackage{amssymb}
   - \usepackage{mathtools}
   - \usepackage{nicefrac}
   - \usepackage{fullpage}
   - \usepackage{stmaryrd}
   - \usepackage{aligned-overset}
   - \usepackage{pdfpages}
# Hvordan får  vi den til at printe February i stedet for februar??????
output: html_document
---

------------------------------------------------------------------------

```{=html}
<!-- RMD tips:
1. CTRL+SHIFT+C RMD-comments-out the selected lines, with in a standard HTML comment-out format. 

2. CTRL+ALT+I inserts a new r codechunck

3. Pressing CTRL+SHIFT+ENTER when over a code chunck gives you a preview of the results of the chunck

4. CTRL+SHIFT+K gives you a preview of the entire resulting HTML file.

5. Note the different results of '#HS 1' and 
'# HS 1' (without the '') in the output

6. It is possible to compile regular R-scripts into Rmd files - this is done by pressing CTRL+SHIFT+K while attending any R-script. <<<- Though this apparently doesn't work for HS Problems.R for some reason!?!?!? ->>>

7. Selective use of the echo=c(...) option within code chuncks allows assignment of a variable, to show the assignment in the knitted document, and showing the value of the assignment seamlessly as well - see HS2.3

8. It is possible to include results of R-analysis such as summary statistics in LaTeX-equations in RMD, see HS2.3

9. Adding fig.align="center" to a code chunk centers any figures generated by the chunck.

9.1 note that properties of codechunks seem casesensitive; fig.align="center" centers a figure, but fig.align="Center" (with capital C) doesn't

10. A new subtitle needs a blank line before itself: 
'works:

#### HS 7
'

'doesn't:
blablabla
#### HS 7
'

'doesn't either:
<!-- blablabla ->
#### HS 7
'
11. Pressing F7 when marking, or hovering over a word will spellcheck the word

12. CTRL + - (minus) zooms out, CTRL + + (plus) zooms in

13. CTRL + D Deletes the current line, or current selection of lines

14. THE FOLLOWING SOURCE EDITOR FOLDING METHODS:
14.1 Collapse current fold: ALT + L
14.1.1: Expand current fold: SHIFT + ALT + L
14.2 Collapse "all" subfolds: ALT + O <- !?!! Note that this leaves a small letter 'o' in the text !!?!
14.2.1 Expand "all" subfolds: SHIFT + ALT + O
14.3 Collapse all other folds: ALT + 0 (zero)

15. SHIFT + ALT + J allows you to jump to specific parts of the document

16. Writing a new line with '...' will cause all previous output to be hidden in the knittet document

17. Writing (q<-5) around R code, will both assign and print the code upon assignment 

18. Note that 'attach' only has the scope of the current R-chunck.

19. One way to get pdf printout is to compile a html-printout, and then, in-browser, 'print' the HTML page as a pdf.

20. CTRL + SHIFT + M gives the pipe operator.

21. Pressing CTRL + F3 searches on the selected word.

22. CTRL

23. ALT + SHIFT + DOWN copies a line to below.
-->
```
<!-- ---?--- How do I create a closeable Rmd section, such that I do not have to scroll through the LaTeX commands each time? - !!! Can be done with '-----' through this also creates a line in the knittet document. -->

<!-- How do I publish and share the HTML as a viewable (and linkable) website - this can be done through github? -->

<!-- How can I share R markdown files such that multiple people can edit them at the same time? -->

<!-- Do we need parindent controls as in LaTeX? -->

<!-- Use of the cache function to reduce recompile times -->

<!-- How to close current subsection with a keyboard shortcut? How to close subsubsections,...? - !!!See RMD tip 14!!! -->

<!-- Chunk naming? -->

<!-- How to define variables such that they have scope within their own ## segment? -->

<!-- How do I delete all non-needed variables for each new section in R??? -->

<!-- LaTeX commands -->

\newcommand{\C}{\mathbb{C}}

<!--- Komplekse tal --->

\newcommand{\R}{\mathbb{R}}

<!--- Reelle tal--->

\newcommand{\Q}{\mathbb{Q}}

<!---Rationelle tal--->

\newcommand{\Z}{\mathbb{Z}}

<!---Hele tal--->

\newcommand{\N}{\mathbb{N}}

<!---Naturlige tal--->

\newcommand{\E}{\mathbb{E}}

<!---mean--->

\newcommand{\F}{\mathbb{F}}

<!---Baggrundsrum sigma-alg--->

\newcommand{\B}{\mathbb{B}}

<!---Borel sigma--->

\newcommand{\K}{\mathbb{K}}

<!---Generel field--->

\newcommand{\RB}{\overline{\R}}

<!---Udvidede reelle tal--->

\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\BR}{\mathcal{B}\left(\R\right)}

<!---Borel på Reelle tal -->

\newcommand{\BRB}{\mathcal{B}\left(\RB\right))}

<!---Borel på udvidede reelle tal -->


\newcommand{\mf}[1]{\mathfrak{#1}} 
\newcommand{\mcG}[2]{\mathcal{#1}^1(#2)} 
\newcommand{\mcGG}[4]{\mathcal{#1}_{#3}^{#2}(#4)}
\newcommand{\GMR}{\left(X,\ms{A},\mu\right)}

<!---Generelt målrum -->

\newcommand{\PBS}{\lrp{\Omega,\F, P}}

<!--- Probability background space -->

\newcommand{\RMR}{\left(\R,\BR, \lambda\right)}

<!---Reelt målrum, m. Borel, og lebesgue mål. -->

<!---L_p spaces on [0,1] with m -->


\newcommand{\Lp}[1]{L_{#1}\lrp{\lrs{0,1},m}} 
\newcommand{\mclxy}{\mc{L}\lrp{X,Y}}
<!---Bounded linear functionals from X to Y -->

\newcommand{\mckxy}{\mc{K}\lrp{X,Y}}

<!---Compact Bounded linear functionals from X to Y -->

\newcommand{\mssr}{\ms{S}(\R)}

<!---The Schwartz space on $\R$ -->

<!---Arrows -->

\newcommand{\ra}{\rightarrow}

<!---Konvergens pil højre -->

\newcommand{\nra}{\nrightarrow}

<!---ikke Konvergens pil højre -->

\newcommand{\la}{\leftarrow}

<!---Konv pil venstre -->

\newcommand{\nla}{\nleftarrow}

<!---ikke Konvergens pil venstre -->

\newcommand{\lra}{\leftrightarrow}

<!---højre venstre pil -->

\newcommand{\nlra}{\nleftrightarrow}

<!---ikke højre venstre pil -->

\newcommand{\hra}{\hookrightarrow}

<!---Injektiv  pil højre -->

\newcommand{\Ra}{\Rightarrow}

<!---Implikations pil højre -->

\newcommand{\Lra}{\Leftrightarrow}

<!---Bi-implikations pil -->

\newcommand{\Uda}{\Updownarrow}

<!---Bi-implikations pil (op og ned) -->

\newcommand{\Da}{\Downarrow}

<!---implikations pil (ned) -->

\newcommand{\rhpu}{\rightharpoonup}

<!---Weak convergence in Hilbert spaces -->

<!-- LHS & RHS calculations -->

\newcommand{\swel}{\overset{\swarrow}{=}}

<!---Continue calculation on left hand side with equality -->

\newcommand{\sweq}{\overset{\swarrow}{\equiv}}

<!---Continue calculation on left hand side with equivalence -->

\newcommand{\seel}{\overset{\searrow}{=}}

<!---Continue calculation on right hand side with equality -->

\newcommand{\seeq}{\overset{\searrow}{\equiv}}

<!---Continue calculation on right hand side with equivalence -->

\newcommand{\inse}{\overset{\cdot}{=}}

<!--- Insert values in calculation -->

\newcommand{\eqd}{\overset{d.}{=}}

\newcommand{\PMX}{\mc{P}\left(X\right)}

<!---Potensmængde af X -->

\newcommand{\comp}{\mathsf{c}}

<!---Set compliment -->

\newcommand{\sm}{\setminus}

<!---mængdedifferens -->

<!--- Parenteser --->

\newcommand{\lrp}[1]{\mathopen{}\left({#1}\right)\mathclose{}}

<!-- \left("STUFF"\right) -->

\newcommand{\lrc}[1]{\mathopen{}\left\{{#1}\right\}\mathclose{}}

<!-- \left\{"STUFF"\right\} -->

\newcommand{\lrs}[1]{\mathopen{}\left[{#1}\right]\mathclose{}}

<!-- \left["STUFF"\right] -->

\newcommand{\lrb}[1]{\mathopen{}\left|{#1}\right|\mathclose{}}

<!-- \left|"STUFF"\right| -->

\newcommand{\inner}[2]{\mathopen{}\left\langle #1, #2 \right\rangle\mathclose{}}

<!-- <\left"STUFF1","STUFF2"\right> -->

\newcommand{\norm}[1]{\mathopen{}\left\lVert#1\right\rVert\mathclose{}}

<!-- \left||"STUFF"\right|| -->

\newcommand{\floor}[1]{\lfloor #1 \rfloor}

<!---Floor function --->

\newcommand{\ceil}[1]{\lceil #1 \rceil}

<!---ceil --->

\newcommand{\FFou}[1]{\mc{F}(#1)}

<!---Fourier Transform notation 1 --->

\newcommand{\Fou}[1]{\widehat{#1}}

<!---Fourier Transform notation 2 --->

<!--- Farver --->

\newcommand{\blue}[1]{\textcolor{blue}{{#1}}}

<!--- Turning text blue --->

\newcommand{\red}[1]{\textcolor{red}{{#1}}}

<!--- Turning text red --->

\newcommand{\green}[1]{\textcolor{green}{{#1}}}

<!--- Turning text green --->

\newcommand{\purple}[1]{\textcolor{purple}{{#1}}}

<!--- Turning text purple --->

\newcommand{\cyan}[1]{\textcolor{cyan}{{#1}}}

<!--- Turning text cyan --->

\newcommand{\orange}[1]{\textcolor{orange}{{#1}}}

<!--- Turning text orange --->

<!--- Oversetting bold accents --->


\newcommand{\boldhat}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\boldbar}[1]{\mathbf{\bar{\text{$#1$}}}}
\newcommand{\boldtilde}[1]{\mathbf{\tilde{\text{$#1$}}}}
\newcommand{\boldcheck}[1]{\mathbf{\check{\text{$#1$}}}}
\newcommand{\indep}{\perp \!\!\! \perp}

<!---independence --->

\newcommand{\colvec}[1]{\begin{pmatrix}{#1}\end{pmatrix}}

<!-- Begin column vector - Doesn't seem to work with non-column vectors...-->

\newcommand{\nd}[2]{\mc{N}\lrp{{#1},{#2}}}

<!-- Normal distribution -->

\newcommand{\dnd}[2]{\sim\mc{N}\lrp{{#1},{#2}}}

<!-- Distributed as Normal distribution -->

\newcommand{\wnd}[3]{\frac{1}{\sqrt{2\pi\cdot {#3}}}e^{-\frac{1}{2}\frac{\lrp{{#1}-{#2}}^2}{{#3}}}}

<!-- With normal density (prob = #1, mean = #2, variance = #3 -->

\newcommand{\wpd}[2]{\frac{{#2}^{{#1}}\cdot e^{-{#2}}}{{#1}\!}}

<!-- With poisson density (prob = #1, mean = #2 -->

\newcommand{\ep}{\varepsilon}

<!-- \newcommand{\Rlogo}{![](R_logo.png){#id .class width=auto height=16px} } <!-- R logo implemented in text -->

<!-- Image insertion alla LaTeX doesn't seem to work too well..., but inserting the above gives the desired effect. -->

<!--???? \declareMathOperator{\SE}{SE} DOESN'T REALLY SEEM TO WORK????-->

<!-- #librar(reshape2) -->

<!-- #librar(lattice) -->

<!-- #librar(hebin) -->

<!-- #librar(xtable) -->

<!-- #librar(splines) -->

<!-- #librar(survival) -->

<!-- #librar(grid) -->

<!-- #librar(lpSolve) -->

<!-- #librar(unit) -->

<!-- #librar(MASS) #NOTE THAT MASS CAN CAUSE CONFLICTS WITH DPLYR OVER SELECT-FUNCTION -->




<!-- # !!!dplyr::select(indNoNA, where(is.numeric)) -->
<!-- indNoNA <- indians %>% drop_na() %>% mutate(across(c(pregnant:insulin,age), as.integer)) -->
<!-- install.packages("ggrepel") -->
<!-- !!! ```{r} -->
<!-- gdf <- -->
<!--   tibble(g = c(1, 1, 2, 3), v1 = 10:13, v2 = 20:23) %>% -->
<!--   group_by(g) -->
<!-- gdf -->

<!-- set.seed(1) -->

<!-- # Outside: 1 normal variate -->
<!-- n <- rnorm(1) -->
<!-- n -->
<!-- gdf %>% mutate(across(v1:v2, ~ .x + n)) -->
<!-- ``` -->

<!-- !!! ```{r} -->
<!-- ? family -->
<!-- ``` -->

<!-- !!! # ```{r} -->
<!-- # ifelse -->
<!-- # ``` -->

<!-- # ```{r} -->
<!-- # df <- df %>% -->
<!-- #   mutate(n = row_number()) %>% #create row number if you dont have one -->
<!-- #   select(n, everything()) # put 'n' at the front of the dataset -->
<!-- # train <- df %>% -->
<!-- #   group_by(var1, var2) %>% #any number of variables you wish to partition by proportionally -->
<!-- #   sample_frac(.7) # '.7' is the proportion of the original df you wish to sample -->
<!-- # test <- anti_join(df, train) # creates test dataframe with those observations not in 'train.' -->
<!-- # ``` -->


<!-- ```{r} -->
<!-- RColorBrewer::brewer.pal(10, "Set1") #but then try with 9 -->
<!-- ``` -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
Sys.setenv("TZ" = "Australia/Sydney")
library(tidyverse)
library(gridExtra)
theme_set(theme_minimal())
library(MASS) #Has a conflict with dplyr regarding select.
library(reshape2)
library(caret) #Used for Machine Learning
#library(plot.matrix) 
library(Matrix)
library(expm)
#library(plotly) #for 3D plotting
#library(reticulate) #Used for saving plotly images
#library(nloptr) #optimisation
#library(lpSolve) #linear programming
library(microbenchmark)
library(scales) #used for log trans of ggplot
#library(stats4) #mle and stuff
#set.seed(314)

varnametotext <- function(v){
   deparse(substitute(v))
}
Stdresplot <- function(model, main = paste("(Estimate, Std. Res.)-plot of", deparse(substitute(model))), ylab ="Standardized residuals", ...) {
 fit <- fitted(model)
 rst <- rstandard(model)
 qplot(fit, rst, main = main, ylab = ylab, ylim = c(-max(3.2,max(abs(rst))), max(3.2,max(abs(rst)))) )+geom_hline(yintercept = 0) #Largest symmetric interval (around 0) of (-3.2,3.2) or (-largest absolute rst, largest absolute rst)
}
QQplotdraw <- function(model, main = paste("Normal QQ-plot of", deparse(substitute(model))), xlab = "Theoretical Quantiles", ylab ="Sample Quantiles", ...) {
   rst <- rstandard(model)
   #dataname <- getCall(lm_LT)$data
   ggplot(data = eval(getCall(model)$data), main = main, xlab = xlab, ylab = ylab) + geom_qq() + geom_qq_line() + aes(sample = rst)
} #main, xlab, ylab call do not work for some reason
StdresQQPlot <- function(model,...) {
   p1 <- Stdresplot(model,...)
   p2 <- QQplotdraw(model,...)
   #library(gridExtra)
   grid.arrange(p1,p2, ncol = 2)
}
```


This document was compiled on: `r format(Sys.time(), "%a %b %d  %X %Y %Z")`

### Instructions

3. This is an open book quiz, and you are allowed to search the Internet and the course webpage to access any descriptions and `R` code that may help you to solve the questions. However, be mindful that searching for the solution may take longer than you may be capable of doing by youself. Keep track of your time regularly.
4. Place your answers in the code chunk spaces provided adding typed comments below the code chunks as necessary if required (some questions may ask you to interpret your output).
5. We suggest you begin by **knitting** (compiling) the Rmd to make sure the template works on your system.
6. It's a good idea to submit your work **regularly** - if you end up submitting the final version late, the marker can go back and mark only the on-time submission.
7. For time management it is suggested you set the alarm on your phone and give yourself plenty of time for submission.
8. A single mark will be awarded if a complete HTML file with compiled R code is submitted.
9. The required packages for this test are `mlbench`, `caret` and `stats4` and can be loaded with

```{r, message = FALSE}
library(tidyverse) #added
library(stats4)
library(caret)
library(mlbench)
```


### Question 1

Construct and analyze a binary classifier from a simulated dataset using your SID and the `mlbench` package.

a. **[4 marks]** Simulate a single dataset of `n = 500` observations completing the code below replacing `<insert SID here>` with your SID respectively and changing the `eval = FALSE` setting to `eval = TRUE` in the `R` code chunk. Then, inspect the `q1.dat` using the `head` command and verify the dimension of the `data.frame` and that there are 2 numeric features and a single factor variable of class labels. __Note__: _You should explicitly verify the data type is numeric or factor or inspect the class of each column_.
```{r 1a, eval = T}
library(mlbench)
SID <- 520540446# INSERT YOUR SID AS AN INTEGER HERE
set.seed(SID)
simulated.data <- mlbench.2dnormals(n = 500, sd = 2)
q1.dat <- as.data.frame(simulated.data)
```

We inspect the head and check the dimension of `q1.dat`
```{r}
q1.dat %>% head() #head
q1.dat %>% dim #dim
```

We in particular note that `head` also showcases the datatypes of each column, for which we see that there are two numeric and one factor column.
Alternatively we may use ``glimpse` which also showcases this
```{r}
q1.dat %>% glimpse()
```

Alternatively again we may also use:
```{r}
sapply(q1.dat, class)
```


b. **[6 marks]** Split the simulated data into 75% training and 25% test data set using `caret::createDataPartition` or otherwise. Then fit a **logistic** regression model to explain the classes response using the two features on the **training data**. Predict the classes on the **test data** using your fitted logistic regression model and the threshold for classification using the estimated probability of being in the positive class at 0.5. Compute the accuracy on the test set in this situation.

We may create the partition, and fit the logistic regression, noting that `y~.` will capture both predictor features.
```{r 1b}
q1TrainIndex <- createDataPartition(q1.dat$classes, p = 0.75, times = 1, list = F)
q1Train <- q1.dat[q1TrainIndex,]
q1Test <- q1.dat[-q1TrainIndex,]

logmodel <- glm(classes ~ ., family = binomial(link = "logit"), data = q1Train)
```

We may note that the first level of `classes` is `2` and `glm` will thus interpret this as `positive` with the logistic regression.
```{r}
levels(q1.dat$classes)
```

We may use `logmodel` for prediction. Note that $>0$ at $X\beta$ level equates to $>0.5$ at $P$-level.
```{r}
#predicting the class
# logitInv <- function(x) {exp(x)/(1+exp(x))}
# logitInv(predict(logmodel, newdata = q1Test))>0.5
log_pred_class <- ifelse(test = predict(logmodel, newdata = q1Test)>0, yes = 2, no = 1) #!!!Don't use as.factor!
mean(log_pred_class == q1Test$classes) #the accuracy
```
We thus get an accuracy of `r mean(log_pred_class == q1Test$classes)` for the model.

c. **[5 marks]** Create a scatter plot of the data in the **test** set and colour each point by its true class label. Draw the linear decision boundary generated by the **logistic** regression model fitted above. Comment on how the accuracy computed on the test set in part c. relates to your plot and decision boundary.
```{r 1d}
# Input answer here

plog <- q1Test %>% ggplot(mapping = aes(x = x.1, y = x.2, colour = classes)) + geom_point(alpha = 0.9) + labs(title = "q1Test data") + scale_color_brewer(palette = "Set1")

log_s_logit_coef <- as.numeric(summary(logmodel)$coef[,1])
(yint_log <- -log_s_logit_coef[1]/log_s_logit_coef[3])
(slo_log <- -log_s_logit_coef[2]/log_s_logit_coef[3])


plog + 
   geom_abline(intercept = yint_log, slope = slo_log, size = 1.2,
               linetype = "dotted", colour = "black")
```

We note that the accuracy of `logmodel` previously computed, relates to the proportion of the points in the above plot, that are correctly classified to be on their respective correct side of the above line. The between the two, is the logit link function via which `logmodel` predicts that all points 'above' the drawn line have a higher than $0.5$ probability of being class $1$ while points 'below' the line are predicted to be class $2.$


### Question 2

Consider the estimation of density of the duration of a geyser eruptions (in minutes). Provided is a messy and clean dataset these geyser eruption durations in the files `clean-s1-22-q2.rds` and `messy-s1-22-q2.rds` respectively. The `readRDS` commands below load the data using the native data format in `R`.

```{r}
messy.duration <- readRDS('messy-s1-22-q2.rds')
clean.duration <- readRDS('clean-s1-22-q2.rds')
```

Suppose only the messy dataset was only available initially and requires cleaning by removing the negative and missing (`NA`) values. The goal here is the clean the messy dataset and then provide an analysis of the stability of the bandwidths in the `density` algorithm.

a. **[4 marks]** Some geyser eruptions were recorded incorrectly with a negative duration or were coded as missing (coded as `NA`). Using relevant `R` code, verify that `messy.duration` is a `numeric` vector with `341` observations. Also count the number of observations that are negative and the number that are coded as missing (i.e. `NA`).
```{r 2a}
# Input answer here
#numeric vector of length 341:
is.vector(messy.duration) #It is a vector
class(messy.duration) #It is numeric
length(messy.duration) #it is of length 341

sum(messy.duration >= 0, na.rm = T) #number of positive vals
sum(messy.duration < 0, na.rm = T) #number of negative vals
sum(is.na(messy.duration)) #number of missing values
```

b. **[3 marks]** Create a new vector called `my.cleaned.duration` which uses the `messy.duration` vector and cleans it by removing the negative values or values that are coded as `NA`. *Verify* that your created vector is the same as the `clean.duration` dataset using a call to `all.equal` or `identical`.
```{r 2b}
#Filtering for positive values is sufficient for the required task
my.cleaned.duration <- messy.duration %>% subset(messy.duration>0)
identical(clean.duration, my.cleaned.duration)
```

***

From this point onwards, answer the questions using only the `clean.duration` dataset.


c. **[6 marks]** Produce a histogram and kernel density estimate of the cleaned data on the *same plot*. Using either the base `R` plotting commands (e.g. `hist`) or using the `ggplot2` comments are fine, no need to use both. Also no need to do bandwidth selection in this question and any default bandwidths are fine.


We start by making a dataframe out of `clean.duration`
```{r 2c}
# Input answer here
df_clean <- data.frame(minutes = clean.duration)
p0 <- df_clean %>% ggplot() + xlab("Minuts") + ylab("Density") + geom_histogram(mapping = aes(x = minutes, y = ..density..), bins = 30, colour = "black", fill = "yellow") + ggtitle("Minutes, Density. 30 bins.")
(p1 <- p0 + geom_density(mapping = aes(x = minutes, y = ..density.., linetype = "kernel Estimate"), kernel = "gaussian", colour = "cyan", size = 0.8, alpha = 0.5, show.legend = T)) #Labs to fix linetype. <- see week 4/5 maybe.
```

d. **[2 marks]** Construct `B = 341` bootstrap samples of the cleaned geyser data by resampling with replacement (i.e. using the functions `sample` or `sample.int`, marks will not be awarded for code that uses an external package like `boot`).
```{r 2d}
# Input answer here
B <- 341
boots = list()
for (i in 1:B) {
   boots[[i]] <- sample(x = clean.duration, replace = T)   
}
```

```{r}
clean.duration
```


e. **[3 marks]** Using your bootstrapped geyser data above, analyze how variable the default bandwidth estimator is on this bootstrapped data. Visualize your results using a boxplot (again using either the base `boxplot` command or `ggplot2` suite are fine, no need to do both). **Hint**: This can be done by extracting the `bw` element of the return output of `density` or by calling the function `bw.nrd0` instead.

We may note that the kernel density estimate provided by `geom_density` uses `bw.nrd0` to select its bandwidth. With this, we may note that the estimated bandwith of the above plot may be captured by `bw.nrd0(clean.duration)` as below
```{r 2e}
# Input answer here
(bw_clean_dur <- bw.nrd0(clean.duration))
bws <- sapply(boots, bw.nrd0)
```

we may reassemble `bws` into a dataframe:
```{r}
bws_df <- data.frame(bandwidth = bws)
bws_df %>% ggplot(aes(x = bandwidth)) + geom_boxplot() + geom_vline(colour = "maroon", size = 1, linetype = "dashed", xintercept = bw_clean_dur, show.legend = T) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) + ggtitle("Histogram of 341 bootstrap estimates of bw.nrd0 from clean, w. original estimate")
```

***

The distribution looks bimodal with two peaks, consider now an alternative way to estimate the density as the mixture of two normal densities. A mixture of two normal densities is a model which is essentially a weighted combination of two standard Gaussian densities, one centered at the first peak and another at a second peak. The weights here denote the chance of beloning to each Gaussian. The associated joint log likelihood of such a model is defined below.
\[
    \mathcal L(\boldsymbol{\theta}| \mathcal X) = \begin{cases} \sum_{i = 1}^n \log \left(p \cdot g(x_i; \mu_1,\sigma_1) + (1 - p)g( x_i; \mu_2, \sigma_2 )\right),& \text{ if } 0 < p < 1 , \sigma_1 > 0 \text{ and } \sigma_2 > 0;\\
               -\infty, & \text{otherwise}.\end{cases}
\]
where $g(x; \mu, \sigma)$ is the normal density with mean $\mu$ and standard deviation of $\sigma$ and $\mathcal X = \left\{ x_1, x_2,\ldots, x_n\right\}$ denotes all the sample data. **Hint**: The function `dnorm` is useful here and the mean and standard deviation can be set with the `mean` and `sd` arguments.

f. **[3 marks]** Define a function in `R` that computes the _negative_ log-likelihood as a function of $\boldsymbol{\theta}$ defined above. (Note $\boldsymbol{\theta} = (p, \mu_1, \sigma_1, \mu_2, \sigma_2)$ has 5 elements and will require a function with five input arguments e.g. `fun <- function(v, w, x, y, z)` has five arguments `v, w, x, y, z`

```{r 2f}
# Input answer here
mixLLH <- function(x, p, mu1, mu2, sigma1, sigma2) {
   if (p<1 & p>0 & sigma1 >0 & sigma2>0) {
      return(sum(log(p*dnorm(x = x, mean = mu1, sd = sigma1, log = F) + (1-p)*dnorm(x = x, mean = mu2, sd = sigma2, log = F))))
   }
   else {
      return(-Inf)
   }
}
mix_minus_LLH_clean <- function(p, mu1, mu2, sigma1, sigma2) {
 return(-mixLLH(x = clean.duration, p = p, mu1 = mu1, mu2 = mu2, sigma1 = sigma1, sigma2 = sigma2))  
}
```

g. **[2 marks]** Compute the MLE of $\boldsymbol{\theta}$ using `stats4::mle`.  **Hint**: The starting point of the algorithm is required with the `start` argument list, choose an appropriate starting point based off the histogram or density estimate computed in part c. e.g. the mean of the two components could be started at the highest peaks in the earlier plot in c.

We evaluate approximate values for $\mu_1, \mu_2, \sigma_1, \sigma_2$ along with $p=0.5$ for start conditions of the optimisation implemented in `stats4::mle`
```{r 2g}
# Input answer here
library(stats4)
mle_est <- stats4::mle(mix_minus_LLH_clean, start = list(p = 0.5, mu1 = 2, sigma1 = 1, mu2 = 4, sigma2 = 1))
mle_est
coef(mle_est)
p <- coef(mle_est)[[1]]
mu1 <- coef(mle_est)[[2]]
mu2 <- coef(mle_est)[[3]]
sigma1 <- coef(mle_est)[[4]]
sigma2 <- coef(mle_est)[[5]]
```

h. **[3 marks]** Produce a **single** plot of the histogram, density estimate computed in part c. and the estimated mixture of two Gaussians at the MLE and comment which seems superior, the MLE using a Gaussian mixture or the density estimator? **Hint** The density of a mixture of two Gaussians, $f$ is defined below.
\[
    f(x; p, \mu_1, \sigma_1, \mu_2, \sigma_2) = p\cdot g( x; \mu_1, \sigma_1) + (1 - p)g( x;  \mu_2,\sigma_2), \qquad \text{ where } 0 < p < 1 \text{ and } \sigma_1 > 0 \text{ and } \sigma_2 > 0.
\]


<!-- # Input answer here -->
<!-- #p1 + stat_function(fun = dnorm, args = list(mean = coef(mle_est)[[2]], sd = coef(mle_est)[[4]])) +  -->
<!-- #stat_function(fun = dnorm, args = list(mean = coef(mle_est)[[4]], sd = coef(mle_est)[[5]])) -->

```{r 2h}
p1 + geom_function(fun = function(x) p * dnorm(x, mean = mu1, sd = sigma1) + (1-p)*dnorm(x, mean = mu2, sd = sigma2), mapping = aes(linetype = "mixture"))
#fix look, but otherwise ok.
```






















